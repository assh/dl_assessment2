{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Practical Assessment - Deep Learning [100 Marks]\n",
    "\n",
    "\n",
    "<div class=\"logos\"><img src=\"Comp5625M_logo.jpg\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "This assessment is divided into two parts:\n",
    "> 1. Image classification using DNN and CNN [70 Marks]\n",
    "> 2. Use of RNN to predict texts for image captioning [30 Marks]\n",
    "\n",
    "The maximum number of marks for each part is shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 50% of the final grade for the module.\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Understand and implement your first deep neural network and convolutional neural network (CNN) and see how these can be used for classification problem \n",
    "> 2. Practice building, evaluating, and finetuning your CNN on an image dataset from development to testing stage. \n",
    "> 3. You will learn to tackle overfitting problem using strategies such as data augmentation and drop out.\n",
    "> 4. Compare your model performance and accuracy with others, such as the leaderboard on Kaggle\n",
    "> 5. Use RNNs to predict the caption of an image from established word vocabularies\n",
    "> 6. Understand and visualise text predictions for a given image.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this provided template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Image Classification [70 marks]\n",
    "\n",
    "#### Dataset\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n",
    "\n",
    ">[Direct access of data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[To submit your results on the Kaggle competition. You can also access data here](https://www.kaggle.com/t/9105198471a3490d9057026d27d8a711)\n",
    "\n",
    "To access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb`` (see the ``note`` below)\n",
    "\n",
    "``Note:`` If the name is already taken in the Kaggle then please use a similar pseudo name and add a note in your submission with the name you have used in the Kaggle. \n",
    "\n",
    "#### Submitting your test result to Kaggle leaderboard \n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. More information is provided in the related section below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed some examples for importing some libraries is provided for you below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview of image classification:**\n",
    "\n",
    "**1. Function implementation** [14 marks]\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (6 marks)\n",
    "\n",
    "**2. Model training** [30 marks]\n",
    "*   **2.1** Training on TinyImageNet30 dataset (6 marks)\n",
    "*   **2.2** Generating confusion matrices and ROC curves (6 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (18 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "            \n",
    "**3. Model testing** [10 marks]\n",
    "*   **3.1**   Testing your final model in (2) on test set - code to do this (4 marks)\n",
    "*   **3.2**   Uploading your result to Kaggle  (6 marks)\n",
    "\n",
    "**4. Model Fine-tuning on CIFAR10 dataset** [16 marks]\n",
    "*   **4.1** Fine-tuning your model (initialise your model with pretrained weights from (2)) (6 marks)\n",
    "*   **4.2** Fine-tuning model with frozen base convolution layers (6 marks)\n",
    "*   **4.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe. (4 marks) \n",
    "\n",
    "\n",
    "<!-- **5. Model comparison** [16 marks]\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (8 marks)\n",
    "*   **5.2**   Compare the results of your model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks) -->\n",
    "<!-- \n",
    "**6. Interpretation of results** (14 marks)\n",
    "*   **6.1** Implement grad-CAM for your model and AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare your results from your model and AlexNet (4 marks)\n",
    "*   **6.3** Provide comment on (4 marks)\n",
    "    - why the network predictions were correct or not correct in your predictions? \n",
    "    - what can you do to improve your results further?\n",
    "\n",
    "**7. Residual connection for deeper network** (9 marks)\n",
    "*   **7.1** Implement a few residual layers in AlexNet and retrain on TinyImageNet30. You can change network size if you wish. (6 marks)\n",
    "*   **7.2** Comment on why such connections are important and why this impacted your results in terms of loss and accuracy (if it did!) (3 marks)\n",
    "\n",
    "**Quality of your report** (2 marks) -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Function implementations [14 marks]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset class (4 marks)\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# TO COMPLETE\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImage30Dataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.classes = [f for f in os.listdir(directory) if not f.startswith('.') and not f.endswith('.txt')]\n",
    "        self.classes.sort()\n",
    "        self.class_to_idx = {self.classes[i]: i for i in range(len(self.classes))}\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith('.JPEG') or img_name.endswith('.png'):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), self.class_to_idx[class_name]))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, target = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImage30TestDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        # Load only JPEG and PNG images and sort them to maintain order\n",
    "        self.images = sorted(\n",
    "            [img for img in os.listdir(directory) if img.endswith('.JPEG') or img.endswith('.png')]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path and load it\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.directory, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply the transformation\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the image tensor and its corresponding filename\n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dir = 'data/train_set'  # Update this path\n",
    "test_dir = 'data/test_set'      # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = TinyImage30Dataset(directory=train_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_size = len(full_dataset)\n",
    "test_size = int(0.2 * full_dataset_size)  # 20% for testing\n",
    "train_size = full_dataset_size - test_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Now create DataLoader instances for the train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (images,labels) in enumerate(val_loader):\n",
    "    print(f\"Batch {i} in val_loader: images shape {images.shape}\")\n",
    "    if i == 1:  # Just check the first couple of batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i} in train_loader: images shape {images.shape}, labels shape {labels.shape}\")\n",
    "    if i == 1:  # Just check the first couple of batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images) in enumerate(test_loader):\n",
    "    print(f\"Batch {i} in train_loader: images shape {images.shape}\")\n",
    "    if i == 1:  # Just check the first couple of batches\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a MLP model class (4 marks)\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a MLP Model class\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_units, hidden_units)  # Adjusted for 64x64 RGB input\n",
    "        self.activation1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_units, 256)\n",
    "        self.fc3 = nn.Linear(256, output_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 64 * 64)  # Flatten the image to a vector\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x = self.activation1(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # First layer\n",
    "        self.fc2 = nn.Linear(512, 256)  # Second layer\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third layer\n",
    "        self.fc4 = nn.Linear(128, num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # No activation function here as we'll use nn.CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_units = 3 * 64 * 64\n",
    "hidden_units = 512\n",
    "output_units = 30\n",
    "\n",
    "# Instantiate the model\n",
    "mlp_model = MLP(input_units=input_units, hidden_units=hidden_units, output_units=output_units)\n",
    "mlp_model_v2 = SimpleMLP(input_size=input_units, num_classes =output_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a CNN model class (6 marks)\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers \n",
    "- Loss function\n",
    "- Optimiser\n",
    "\n",
    "*Please note that the network should be at least a few layers for the model to perform well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a CNN Model class\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # Use 3 for RGB images\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjust for the output size after convolutions and pooling\n",
    "        self.fc2 = nn.Linear(128, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)  # Flatten the output\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training [30 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN model - show loss and accuracy graphs side by side (6 marks)\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. A top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HelperDL function) -- Define top-*k* accuracy (**new**)\n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running your MLP model class\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    # Set the device to GPU if available\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    # Track loss and accuracy values for plotting\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.float() / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Statistics\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_corrects.float() / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1} - '\n",
    "              f'Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.4f} - '\n",
    "              f'Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}')\n",
    "\n",
    "        # Record the loss and accuracy\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss over Epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy over Epochs')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24 - Train Loss: 3.1239 - Train Acc: 0.1207 - Val Loss: 2.9897 - Val Acc: 0.1596\n",
      "Epoch 1/24 - Train Loss: 2.9060 - Train Acc: 0.1694 - Val Loss: 2.9350 - Val Acc: 0.1589\n",
      "Epoch 2/24 - Train Loss: 2.8231 - Train Acc: 0.1892 - Val Loss: 2.9076 - Val Acc: 0.1719\n",
      "Epoch 3/24 - Train Loss: 2.7588 - Train Acc: 0.2020 - Val Loss: 2.8605 - Val Acc: 0.1800\n",
      "Epoch 4/24 - Train Loss: 2.6966 - Train Acc: 0.2194 - Val Loss: 2.8480 - Val Acc: 0.1859\n",
      "Epoch 5/24 - Train Loss: 2.6410 - Train Acc: 0.2434 - Val Loss: 2.8147 - Val Acc: 0.2026\n",
      "Epoch 6/24 - Train Loss: 2.5873 - Train Acc: 0.2480 - Val Loss: 2.8194 - Val Acc: 0.1944\n",
      "Epoch 7/24 - Train Loss: 2.5378 - Train Acc: 0.2631 - Val Loss: 2.8517 - Val Acc: 0.1900\n",
      "Epoch 8/24 - Train Loss: 2.4924 - Train Acc: 0.2711 - Val Loss: 2.7994 - Val Acc: 0.2111\n",
      "Epoch 9/24 - Train Loss: 2.4496 - Train Acc: 0.2879 - Val Loss: 2.8203 - Val Acc: 0.2033\n",
      "Epoch 10/24 - Train Loss: 2.4007 - Train Acc: 0.3012 - Val Loss: 2.8395 - Val Acc: 0.2030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mlp_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(mlp_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train MLP model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mlp_model, mlp_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Plot training history for MLP\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plot_training_history(mlp_history)\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Zero the parameter gradients\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mTinyImage30Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     21\u001b[0m     img_path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[0;32m---> 22\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    876\u001b[0m ):\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/PIL/ImageFile.py:271\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.11/site-packages/PIL/JpegImagePlugin.py:417\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes):\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(read_bytes)\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your graph\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train MLP model\n",
    "mlp_model, mlp_history = train_model(mlp_model, train_loader, val_loader, mlp_criterion, mlp_optimizer, num_epochs=25)\n",
    "\n",
    "# Plot training history for MLP\n",
    "plot_training_history(mlp_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running your CNN model class\n",
    "# Do the same for the CNN model with its own criterion and optimizer\n",
    "cnn_model = SimpleCNN()\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=25)\n",
    "\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on your model and the results you have obtained. This should include the number of parameters for each of your models and briefly explain why one should use CNN over MLP for the image classification problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves (6 marks)\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n",
    "- Display an ROC curve for the two top and two bottom classes with area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "# Assuming you have a trained model 'cnn_model' and DataLoaders 'train_loader' and 'val_loader'  # Set the model to evaluation mode\n",
    "# Function to get predictions and true labels\n",
    "def get_all_preds_labels( loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "      # Ensure the model is in evaluation mode and on the MPS device\n",
    "    model = cnn_model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            # Ensure inputs and labels are floats and on the MPS device\n",
    "            inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())  # Transfer preds back to CPU for numpy conversion\n",
    "            all_labels.append(labels.cpu().numpy())  # Transfer labels back to CPU for numpy conversion\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "train_preds, train_labels = get_all_preds_labels(train_loader)\n",
    "val_preds, val_labels = get_all_preds_labels(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train predictions shape: {train_preds.shape}, Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Validation predictions shape: {val_preds.shape}, Validation labels shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conf_mat = confusion_matrix(train_labels, train_preds)\n",
    "val_conf_mat = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.heatmap(train_conf_mat, annot=True, ax=ax[0], fmt='g')  # Annotate cells with 'g' to disable scientific notation\n",
    "ax[0].set_title('Training Set Confusion Matrix')\n",
    "ax[0].set_xlabel('Predicted labels')\n",
    "ax[0].set_ylabel('True labels')\n",
    "\n",
    "sns.heatmap(val_conf_mat, annot=True, ax=ax[1], fmt='g')\n",
    "ax[1].set_title('Validation Set Confusion Matrix')\n",
    "ax[1].set_xlabel('Predicted labels')\n",
    "ax[1].set_ylabel('True labels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the labels for multiclass ROC curve\n",
    "n_classes = cnn_model.fc2.out_features  # Change 'fc2' to your output layer's name\n",
    "y_bin_train = label_binarize(train_labels, classes=range(n_classes))\n",
    "y_bin_val = label_binarize(val_labels, classes=range(n_classes))\n",
    "\n",
    "# Get probabilities instead of predicted labels\n",
    "def get_all_probs(loader):\n",
    "    all_probs = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = cnn_model(inputs)\n",
    "            all_probs = torch.cat((all_probs, outputs.cpu()), dim=0)\n",
    "    return all_probs.numpy()\n",
    "\n",
    "train_probs = get_all_probs(train_loader)\n",
    "val_probs = get_all_probs(val_loader)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class in training set\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin_train[:, i], train_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for a specific class, e.g., the first and last class\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot(fpr[n_classes-1], tpr[n_classes-1], color='blue', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[n_classes-1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redesign your CNN model (optional)\n",
    "> This is optional and does not carry any marks. Often to tackle model underfitting we tend to make more complex network design. Depending on your observation, you can improve your model if you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_Improved(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(SimpleCNN_Improved, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=128 * 8 * 8, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten the layer before feeding it into fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN_Improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train CNN model\n",
    "model, cnn_history = train_model(model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=15)\n",
    "\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: All questions below here relates to the CNN model only and not an MLP model! You are advised to use your final CNN model only for each of the questions below.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strategies for tackling overfitting (18 marks)\n",
    "Using your (final) CNN model perform the strategies below to avoid overfitting problems. You can resuse the network weights from previous training, often referred to as ``fine tuning``. \n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "> Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data augmentation (6 marks)\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations. \n",
    "\n",
    "> Provide graphs and comment on what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Geometric: Horizontally flip the image with a 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Geometric: Vertically flip the image with a 50% probability\n",
    "    transforms.RandomRotation(30),           # Geometric: Rotate the image up to 30 degrees\n",
    "    transforms.RandomResizedCrop(64),        # Geometric: Crop the image to a random size and aspect ratio\n",
    "    transforms.RandomAffine(15),             # Geometric: Apply a random affine transformation\n",
    "    transforms.RandomPerspective(),          # Geometric: Apply a random perspective transformation\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),  # Randomly erase rectangular portions of the image,\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])  # Color: Adjust brightness, contrast, saturation, and hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_full_dataset = TinyImage30Dataset(directory=train_dir, transform=data_transforms)\n",
    "full_dataset_size = len(full_dataset)\n",
    "test_size = int(0.2 * full_dataset_size)  # 20% for testing\n",
    "train_size = full_dataset_size - test_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Now create DataLoader instances for the train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = SimpleCNN_Improved()\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=25)\n",
    "\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dropout (6 marks)\n",
    "\n",
    "> Implement dropout in your model \n",
    "\n",
    "> Provide graphs and comment on your choice of proportion used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Dropout layer after convolutional layers\n",
    "        self.fc1 = nn.Linear(in_features=128 * 8 * 8, out_features=512)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout layer before the last fully connected layer\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten the layer before feeding it into fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleCNNWithDropout(num_classes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train CNN model\n",
    "model, cnn_history = train_model(model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=50)\n",
    "\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Hyperparameter tuning (6 marks)\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001].\n",
    "\n",
    "> Provide graphs each for loss and accuracy at three different learning rates in a single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = SimpleCNNWithDropout()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.1)\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=7)\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = SimpleCNNWithDropout()\n",
    "# Your graph\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.01)\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=7)\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = SimpleCNNWithDropout()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\n",
    "# Train CNN model\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, cnn_criterion, cnn_optimizer, num_epochs=7)\n",
    "# Plot training history for CNN\n",
    "plot_training_history(cnn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model testing [10 marks]\n",
    "Online evaluation of your model performance on the test set. \n",
    "\n",
    "> Prepare the dataloader for the testset.\n",
    "\n",
    "> Write evaluation code for writing predictions.\n",
    "\n",
    "> Upload it to Kaggle submission page (6 marks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Test class and predictions (4 marks)\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TinyImage30TestDataset(directory=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle  (6 marks)\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, e.g., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: Id and Category (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "The Id column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image on test set and 1 row for the headers. [To submit please click here.](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [6 marks]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! \n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Let's assume you have a test DataLoader named `test_loader`\n",
    "# And a model named `model` that's already trained and moved to the appropriate device\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize lists to store the test ids and predicted categories\n",
    "test_ids = []\n",
    "pred_categories = []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # Assuming the dataset returns a tuple of images and ids\n",
    "        images, ids = data\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Store results\n",
    "        test_ids.extend(ids)\n",
    "        pred_categories.extend(predicted.cpu().numpy().tolist())\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'Category': pred_categories\n",
    "})\n",
    "\n",
    "# Ensure the file name is your student username as per the competition rules\n",
    "submission_file_name = 'mm23ap.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission_df.to_csv(submission_file_name, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [16 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task. You can perform fine-tuning in following fashion:\n",
    "\n",
    "- Train an entire model: Start training model from scratch (large dataset, more computation)\n",
    "\n",
    "- Train some layers, freeze others: Lower layer features are general (problem independent) while higher layer features are specific (problem dependent  freeze)\n",
    "\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation) \n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10`` [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here is only 10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! \n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(cifar10_train))\n",
    "val_size = len(cifar10_train) - train_size\n",
    "train_dataset, val_dataset = random_split(cifar10_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! \n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 10)\n",
    "alexnet = alexnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Apply transfer learning with pretrained model weights (6 marks)\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters\n",
    "alexnet, alexnet_history = train_model(alexnet, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "# Plot training history for CNN\n",
    "plot_training_history(alexnet_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers (6 marks)\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters\n",
    "alexnet_frozen = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "for param in alexnet_frozen.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "alexnet_frozen.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 10)\n",
    "alexnet_frozen = alexnet_frozen.to(device)\n",
    "\n",
    "optimizer = optim.Adam(alexnet_frozen.classifier.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_frozen, alexnet_frozen_history = train_model(alexnet_frozen, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "# Plot training history for CNN\n",
    "plot_training_history(alexnet_frozen_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compare above configurations and comment on performances. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell\n",
    "\n",
    "train_losses_no_freeze = alexnet_history['train_loss']\n",
    "val_losses_no_freeze = alexnet_history['val_loss']\n",
    "train_losses_frozen = alexnet_frozen_history['train_loss']\n",
    "val_losses_frozen = alexnet_frozen_history['val_loss']\n",
    "train_accuracies_no_freeze = alexnet_history['train_acc']\n",
    "val_accuracies_no_freeze = alexnet_history['val_acc']\n",
    "train_accuracies_frozen = alexnet_frozen_history['train_acc']\n",
    "val_accuracies_frozen = alexnet_frozen_history['val_acc']\n",
    "epochs = range(1, len(train_losses_no_freeze) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses_no_freeze, label='Training Loss - No Freeze')\n",
    "plt.plot(epochs, val_losses_no_freeze, label='Validation Loss - No Freeze')\n",
    "plt.plot(epochs, train_losses_frozen, label='Training Loss - Frozen Base')\n",
    "plt.plot(epochs, val_losses_frozen, label='Validation Loss - Frozen Base')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies_no_freeze, label='Training Accuracy - No Freeze')\n",
    "plt.plot(epochs, val_accuracies_no_freeze, label='Validation Accuracy - No Freeze')\n",
    "plt.plot(epochs, train_accuracies_frozen, label='Training Accuracy - Frozen Base')\n",
    "plt.plot(epochs, val_accuracies_frozen, label='Validation Accuracy - Frozen Base')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Image Captioning using RNN [30 marks]\n",
    "\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this part of assessment you will:\n",
    "\n",
    "> 1. Understand the principles of text pre-processing and vocabulary building (provided).\n",
    "> 2. Gain experience working with an image to text model.\n",
    "> 3. Use and compare text similarity metrics for evaluating an image to text model, and understand evaluation challenges.\n",
    "\n",
    "#### Dataset\n",
    "This assessment will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of nearly 5070 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5070.\n",
    "\n",
    "To download the data:\n",
    "\n",
    "> 1. **Images and annotations**: download the zipped file provided in the link here as [``COMP5625M_data_assessment_2.zip``](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/EWWzE-_AIrlOkvOKxH4rjIgBF_eUx8KDJMPKM2eHwCE0dg?e=DdX62H). \n",
    "\n",
    "``Info only:`` To understand more about the COCO dataset you can look at the [download page](https://cocodataset.org/#download). We have already provided you with the \"2017 Train/Val annotations (241MB)\" but our image subset consists of fewer images compared to orginial COCO dataset. So, no need to download anything from here! \n",
    "\n",
    "> 2. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
    "\n",
    "#### Submission\n",
    "\n",
    "You can either submit the same file or make a two separate .ipython notebook files zipped in the submission (please name as ``yourstudentusername_partI.ipynb`` and ``yourstudentusername_partII.ipynb``). \n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet50](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
    "\n",
    "You can alternatively use the COCO trained pretrained weights from [PyTorch](https://pytorch.org/vision/stable/models.html). One way to do this is use the \"FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\" but use e.g., \"resnet_model = model.backbone.body\". Alternatively, you can use the checkpoint from your previous coursework where you fine-tuned to COCO dataset. \n",
    "\n",
    "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by *a batch normalisation layer to speed up training*. Those resulting features, as well as the reference text captions, are then passed into a recurrent network (we will use an **RNNs** in this assessment). \n",
    "\n",
    "The reference captions used to compute loss are represented as numerical vectors via an **embedding layer** whose weights are learned during training.\n",
    "\n",
    "<!-- ![Encoder Decoder](comp5625M_figure.jpg) -->\n",
    "\n",
    "<div>\n",
    "<center><img src=\"comp5625M_figure.jpg\" width=\"1000\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for creating vocabulary \n",
    "\n",
    "A helper function file ``helperDL.py`` is provided that includes all the functions that will do the following for you. You can easily import these functions in the exercise, most are already done for you!  \n",
    "\n",
    "> 1. Extracting image features (a trained checkpoint is provided ``resnet50_caption.pt`` for you to download and use it for training your RNN)\n",
    "> 2. Text preparation of training and validation data is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature map provided to you\n",
    "features_map = torch.load('data/resnet50_caption.pt', map_location=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Train DecoderRNN [20 marks]\n",
    "\n",
    "> 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "> 5.2 Train your model with precomputed features (10 Marks)\n",
    "\n",
    "##### 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/COMP5625M_data_assessment_2/coco/annotations2017/captions_train2017.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_subset = pd.read_csv(\"data/COMP5625M_data_assessment_2/coco_subset_meta.csv\")\n",
    "new_data = pd.DataFrame( data['annotations'])\n",
    "new_coco = coco_subset.rename(columns={'id':'image_id'})\n",
    "new_coco.drop_duplicates('file_name',keep='first',inplace=True)\n",
    "\n",
    "new_subset = new_data.sort_values(['image_id'], ascending=True)\n",
    "# Get all the reference captions\n",
    "new_file = pd.merge(new_coco,new_subset,how = 'inner', on = 'image_id')\n",
    "new_file = new_file[['image_id','id','caption','file_name']]\n",
    "new_file = new_file.sort_values(['image_id'], ascending = True)\n",
    "new_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the clearn clean - e.g., converting all uppercases to lowercases\n",
    "new_file[\"clean_caption\"] = \"\"\n",
    "from helperDL import gen_clean_captions_df\n",
    "new_file = gen_clean_captions_df(new_file)\n",
    "new_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Spilt your training, validation and test dataset with indexes to each set\n",
    "from helperDL import split_ids\n",
    "train_id, valid_id, test_id = split_ids(new_file['image_id'].unique())\n",
    "print(\"training:{}, validation:{}, test:{}\".format(len(train_id), len(valid_id), len(test_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = new_file.loc[new_file['image_id'].isin(train_id)]\n",
    "valid_set = new_file.loc[new_file['image_id'].isin(valid_id)]\n",
    "test_set = new_file.loc[new_file['image_id'].isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build vocabulariy for each set - train, val and test \n",
    "# you will be using to create dataloaders\n",
    "from helperDL import build_vocab\n",
    "\n",
    "# create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "vocab_train = build_vocab(train_id, new_file, vocab)\n",
    "vocab_valid = build_vocab(valid_id, new_file, vocab)\n",
    "vocab_test = build_vocab(test_id, new_file, vocab)\n",
    "\n",
    "vocab = vocab_train # using only training samples as vocabulary as instructed above\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They can also join the train and valid captions but they will need to run vocabulary after concatenation\n",
    "import numpy as np\n",
    "vocab = build_vocab(np.concatenate((train_id, valid_id), axis=0), new_file, vocab) #---> overrighting\n",
    "len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a ```DataLoader``` for your image feature and caption dataset. ``helperDL.py`` file includes all the required functions\n",
    "\n",
    "We need to overwrite the default PyTorch collate_fn() because our \n",
    "ground truth captions are sequential data of varying lengths. The default\n",
    "collate_fn() does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here:\n",
    "https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperDL import EncoderCNN  \n",
    "model = EncoderCNN() \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Preparing the train, val and test dataloaders\n",
    "from helperDL import COCO_Features\n",
    "from helperDL import caption_collate_fn\n",
    "\n",
    "\n",
    "# Create a dataloader for train\n",
    "dataset_train = COCO_Features(\n",
    "    df=train_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")\n",
    "\n",
    "# Create a dataloader for valid\n",
    "dataset_valid = COCO_Features(\n",
    "    df=valid_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say this is as below \n",
    "# --> Please change these numbers as required. \n",
    "# --> Please comment on changes that you do.\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 2\n",
    "LR = 0.0001\n",
    "NUM_EPOCHS = 15\n",
    "LOG_STEP = 10\n",
    "MAX_SEQ_LEN = 37"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # we want a specific output size, which is the size of our embedding, so\n",
    "        # we feed our extracted features from the last fc layer (dimensions 1 x 2048)\n",
    "        # into a Linear layer to resize\n",
    "        # your code\n",
    "        self.resize = nn.Linear(2048, embed_size) # Assuming features are 2048 in size\n",
    "        \n",
    "        \n",
    "        # batch normalisation helps to speed up training\n",
    "        # your code\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "        # your code for embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # your code for RNN\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        im_features = self.resize(features)\n",
    "        im_features = self.bn(im_features)\n",
    "        \n",
    "        # compute your feature embeddings\n",
    "        # your code\n",
    "        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)\n",
    "    \n",
    "        # pack_padded_sequence returns a PackedSequence object, which contains two items: \n",
    "        # the packed data (data cut off at its true length and flattened into one list), and \n",
    "        # the batch_sizes, or the number of elements at each sequence step in the batch.\n",
    "        # For instance, given data [a, b, c] and [x] the PackedSequence would contain data \n",
    "        # [a, x, b, c] with batch_sizes=[2,1,1].\n",
    "\n",
    "        # your code [hint: use pack_padded_sequence]\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.rnn(packed)\n",
    "    \n",
    "\n",
    "\n",
    "        outputs = self.linear(hiddens[0]) #hint: use a hidden layers in parenthesis\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "\n",
    "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate decoder\n",
    "decoder = DecoderRNN(len(vocab), embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Train your model with precomputed features (10 marks)\n",
    "\n",
    "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
    "\n",
    "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
    "\n",
    "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
    "\n",
    "Display a graph of training and validation loss over epochs to justify your stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss and optimizer here\n",
    "# your code here --->\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (using Adam here, which is a common choice)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "# train the models\n",
    "total_step = len(train_loader)\n",
    "total_step_v = len(valid_loader)\n",
    "stats = np.zeros((NUM_EPOCHS,2))\n",
    "print(stats.shape)\n",
    "total_loss = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (features_, captions_, lengths_) in enumerate(train_loader):\n",
    "        # your code here --->\n",
    "        features_ = features_.to(device)\n",
    "        captions_ = captions_.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = decoder(features_, captions_, lengths_)\n",
    "\n",
    "        targets = pack_padded_sequence(captions_, lengths_, batch_first=True)[0]\n",
    "        loss = criterion(outputs, targets) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print stats\n",
    "        if i % LOG_STEP == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    stats[epoch,0] = round(total_loss/total_step,3)\n",
    "    total_loss = 0\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():  \n",
    "        for i, (features_, captions_, lengths_) in enumerate(valid_loader):\n",
    "            # your code here --->\n",
    "            features_ = features_.to(device)\n",
    "            captions_ = captions_.to(device)\n",
    "\n",
    "            outputs = decoder(features_, captions_, lengths_)\n",
    "\n",
    "            targets = pack_padded_sequence(captions_, lengths_, batch_first=True)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            \n",
    "    stats[epoch,1] = round(total_loss/total_step_v,3)\n",
    "    total_loss = 0\n",
    "    # print stats\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train_Loss: {stats[epoch,0]}, Valid_Loss: {stats[epoch,1]}\")\n",
    "    print(\"=\"*30)\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(stats[:,0], 'r', label = 'training', )\n",
    "plt.plot(stats[:,1], 'g', label = 'validation' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title(f\"COCO dataset- train vocab only #vocab={len(vocab)} - 5 epochs\")\n",
    "fig.savefig(\"coco_train_vocab_only.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model after training\n",
    "decoder_ckpt = torch.save(decoder, \"coco_subset_assessment_decoder.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Test prediction and evaluation [10 marks] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Generate predictions on test data (4 marks)\n",
    "\n",
    "Display 4 sample test images containing different objects, along with your models generated captions and all the reference captions for each.\n",
    "\n",
    "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class COCOImagesDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing image paths and captions.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.pth = 'data/COMP5625M_data_assessment_2/coco/images'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 3]  # assuming image path is in the first column\n",
    "        caption = self.df.iloc[idx, 4]   # assuming caption is in the second column\n",
    "\n",
    "        img_path = os.path.join(self.pth, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),     \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset_test = COCOImagesDataset(\n",
    "    df=test_set,\n",
    "    transform=data_transform,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set.iloc[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "decoder.eval() # generate caption, eval mode to not influence batchnormncoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting functions from helperDL.py\n",
    "from helperDL import timshow\n",
    "from helperDL import decode_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_TO_SHOW = 10\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    for i, (image,filename) in enumerate(test_loader):\n",
    "        \n",
    "        # your code here --->\n",
    "        image = image.to(device)\n",
    "\n",
    "        features = model(image)\n",
    "        sampled_ids = decoder.sample(features)\n",
    "        sampled_ids = sampled_ids.cpu().numpy()\n",
    "        \n",
    "        idy = 0\n",
    "        for i in range(sampled_ids.shape[0]):\n",
    "    # Extract the i-th sample's predicted IDs\n",
    "            predicted_ids = sampled_ids[i]\n",
    "    \n",
    "    # Remove the <start>, <end>, and <pad> tokens (assuming their IDs are known)\n",
    "            predicted_ids_trimmed = predicted_ids[(predicted_ids != vocab('<start>')) & (predicted_ids != vocab('<end>')) & (predicted_ids != vocab('<pad>'))]\n",
    "            print(predicted_ids_trimmed)\n",
    "    # Decode the caption\n",
    "            generated_caption = decode_caption(predicted_ids_trimmed, vocab)\n",
    "            idy += 1\n",
    "            if idx == 1:\n",
    "                break\n",
    "    \n",
    "    # Display the generated caption\n",
    "            print(f\"GENERATED: {generated_caption}\\n\")\n",
    "            break\n",
    "\n",
    "        print(f\"REFERENCES: {filename[0]}\\n\")\n",
    "        \n",
    "\n",
    "        print(\"===================================\\n\")\n",
    "\n",
    "\n",
    "        timshow(image[0].cpu())\n",
    "        print(\"===================================\\n\")\n",
    "        idx +=1\n",
    "        if idx == IMAGES_TO_SHOW:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Caption evaluation using cosine similarity (6 marks)\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
    "\n",
    "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
    "\n",
    "* Find the embedding vector of each word in the caption \n",
    "* Compute the average vector for each caption \n",
    "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
    "* Compute the average of these scores \n",
    "\n",
    "Calculate the cosine similarity using the model's predictions over the whole test set. \n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
    "\n",
    "# First, get the embeddings for each word in your vocabulary\n",
    "word_embeddings = decoder.embed.weight.data.cpu().numpy()\n",
    "\n",
    "def get_caption_embedding(caption, vocab, word_embeddings):\n",
    "    # Convert caption to list of indices\n",
    "    indices = [vocab.word2idx[word] for word in caption.split() if word in vocab.word2idx]\n",
    "    # Retrieve the corresponding embeddings and take the mean\n",
    "    if len(indices) == 0:\n",
    "        return np.zeros(word_embeddings.shape[1])\n",
    "    caption_embedding = np.mean(word_embeddings[indices], axis=0)\n",
    "    return caption_embedding\n",
    "\n",
    "cosine_scores = []\n",
    "\n",
    "# Loop through the test dataset\n",
    "for images, true_captions in test_loader.dataset:\n",
    "\n",
    "    if images.ndim == 3:\n",
    "        images = images.unsqueeze(0)  # Add batch dimension at the beginning\n",
    "\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Generate captions for the images\n",
    "    features = model(images)\n",
    "    sampled_ids = decoder.sample(features)\n",
    "    sampled_ids = sampled_ids.cpu().numpy()\n",
    "\n",
    "\n",
    "    # Decode the generated captions\n",
    "    generated_captions = [decode_caption(sample, vocab) for sample in sampled_ids]\n",
    "    generated_embeddings = np.array([get_caption_embedding(cap, vocab, word_embeddings) for cap in generated_captions])\n",
    "\n",
    "    # Get embeddings for the true captions\n",
    "    true_embeddings = np.array([get_caption_embedding(cap, vocab, word_embeddings) for cap in true_captions])\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    for gen_emb, true_emb in zip(generated_embeddings, true_embeddings):\n",
    "        similarity = sklearn_cosine_similarity([gen_emb], [true_emb])\n",
    "        cosine_scores.append(similarity.item())\n",
    "\n",
    "# Display a histogram of the distribution of cosine similarity scores\n",
    "plt.hist(cosine_scores, bins=20)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarity Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thank you for completing the assessment - if you have any question, please ask on teams channel or attend lab sessions on Tuesdays and Wednesdays."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
